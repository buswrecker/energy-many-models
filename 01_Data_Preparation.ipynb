{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "---\n",
    "\n",
    "This notebook creates simulated solar and energy consumption from a real-home dataset in Australia to walk you through the process of training many models and forecasting on Azure Machine Learning.\n",
    "\n",
    "This notebook walks you through all the necessary steps to configure the data for this solution accelerator, including:\n",
    "\n",
    "1. Generate the sample data\n",
    "2. Split in training/forecasting sets\n",
    "3. Connect to your workspace and upload the data to its Datastore\n",
    "\n",
    "### Prerequisites\n",
    "If you have already run the [00_Setup_AML_Workspace](00_Setup_AML_Workspace.ipynb) notebook you are all set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Generate sample data\n",
    "\n",
    "The creation of the synthetic datasets are as follows\n",
    "\n",
    "1. We will create datasets for 5 suburbs; each suburb will have 10 homes each - total of 50 models to build/train upon\n",
    "2. For each home; we will create shift the Solar/Temp/General Usage values with a random normal variable with the means/std of the original dataset\n",
    "3. This would produce 50 separate files in a folder; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw dataset\n",
    "df = pd.read_csv(\"data/sampleEnergy.csv\")\n",
    "\n",
    "# Split reference data into solar and general usage and join\n",
    "general_usage_df = df[df[\"RateTypeDescription\"] == \"Generalusage\"][[\"EndDate\", \"ProfileReadValue\"]].reset_index(drop=True)\n",
    "solar_df = df[df[\"RateTypeDescription\"] == \"Solar\"][[\"EndDate\", \"ProfileReadValue\", \"DeviceNumber\", \"QualityFlag\", \"BOMTEMP\"]].reset_index(drop=True)\n",
    "\n",
    "pd.concat([general_usage_df, solar_df], axis=1, join=\"inner\")\n",
    "\n",
    "# process reference dataset\n",
    "processed_df = solar_df.copy()\n",
    "processed_df[\"Generalusage\"] = general_usage_df[\"ProfileReadValue\"]\n",
    "processed_df[\"Solar\"] = solar_df[\"ProfileReadValue\"]\n",
    "processed_df[\"Temp\"] = solar_df[\"BOMTEMP\"]\n",
    "processed_df[\"NetEnergy\"] = processed_df[\"Solar\"] - processed_df[\"Generalusage\"]\n",
    "processed_df[\"EndDate\"] = pd.to_datetime(processed_df[\"EndDate\"], format=\"%d/%m/%Y %H:%M\")\n",
    "processed_df.drop([\"ProfileReadValue\", \"BOMTEMP\"], axis=1, inplace=True)\n",
    "\n",
    "# Create time-based features\n",
    "processed_df[\"Quarter\"] = processed_df[\"EndDate\"].dt.quarter\n",
    "processed_df[\"Month\"] = processed_df[\"EndDate\"].dt.month\n",
    "processed_df[\"Weekday\"] = processed_df[\"EndDate\"].dt.weekday\n",
    "processed_df[\"Hour\"] = processed_df[\"EndDate\"].dt.hour\n",
    "processed_df[\"WeekOfTheMonth\"] = processed_df[\"EndDate\"].dt.week\n",
    "processed_df[\"Weekend\"] = (processed_df[\"Weekday\"] >= 5).astype(np.int)\n",
    "processed_df[\"DateOfMonth\"] = ((processed_df[\"EndDate\"].dt.day // 7) + 1)\n",
    "processed_df[\"AMPM\"] = (processed_df[\"EndDate\"].dt.hour>11).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to write data\n",
    "folder_path = \"data/synthetic/\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Create simulated suburbs and homes\n",
    "suburbs = [\"Manly\", \"Bondi\", \"StKildas\", \"AlbertPark\", \"NorthBridge\"]\n",
    "homes = list(range(1, 11, 1))\n",
    "\n",
    "# Calculate mean / std of changes for Generalusage, Solar, Temp to augment baseline data\n",
    "generalusage_mean, generalusage_std = processed_df[\"Generalusage\"].diff().mean(), processed_df[\"Generalusage\"].diff().std()\n",
    "solar_mean, solar_std = processed_df[\"Solar\"].diff().mean(), processed_df[\"Solar\"].diff().std()\n",
    "temp_mean, temp_std = processed_df[\"Temp\"].diff().mean(), processed_df[\"Temp\"].diff().std()\n",
    "\n",
    "# Generate synthetic data\n",
    "for suburb_idx, suburb in enumerate(suburbs):\n",
    "    temp_delta = np.random.normal(temp_mean, temp_std, processed_df.shape[0])\n",
    "    \n",
    "    for home_idx, home in enumerate(homes):\n",
    "        suburb_name = suburb\n",
    "        home_name = f\"home{home}\"\n",
    "    \n",
    "        suburb_home_df = processed_df.copy()\n",
    "        suburb_home_df[\"DeviceNumber\"] += suburb_idx + home_idx\n",
    "        suburb_home_df[\"Temp\"] += temp_delta\n",
    "        suburb_home_df[\"Generalusage\"] += temp_delta\n",
    "        suburb_home_df[\"Solar\"] += temp_delta\n",
    "        suburb_home_df.loc[suburb_home_df[\"Generalusage\"] < 0, \"Generalusage\"] = 0\n",
    "        suburb_home_df.loc[suburb_home_df[\"Solar\"] < 0, \"Solar\"] = 0\n",
    "#         suburb_home_df[\"NetEnergy\"] = suburb_home_df[\"Solar\"] - suburb_home_df[\"Generalusage\"]\n",
    "        suburb_home_df[\"Suburb\"] = suburb_name\n",
    "        suburb_home_df[\"Home\"] = home_name\n",
    "        \n",
    "        print(f\"Writing synthetic data for {suburb_name} {home_name}\")\n",
    "        suburb_home_df.to_csv(folder_path + f\"{suburb_name}_{home_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Split data in two sets\n",
    "\n",
    "We will now split each dataset in two parts: one will be used for training, and the other will be used for simulating batch forecasting. The training files will contain the data records before '2020-06-01' and the last part of each series will be stored in the inferencing files.\n",
    "\n",
    "Finally, we will upload both sets of data files to the Workspace's default [Datastore](https://docs.microsoft.compython/api/azureml-core/azureml.core.datastore(class))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helper import split_data\n",
    "\n",
    "# Split each file and store in corresponding directory\n",
    "train_path, inference_path = split_data(folder_path, 'EndDate', '2020-06-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Upload data to Datastore in AML Workspace\n",
    "\n",
    "In the [setup notebook](00_Setup_AML_Workspace.ipynb) you created a [Workspace](https://docs.microsoft.com/python/api/azureml-core/azureml.core.workspace.workspace). We are going to register the data in that enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Take a look at Workspace\n",
    "ws.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a new Datastore and upload data into that data store. Feel free to read the [Datastore](https://docs.microsoft.com/azure/machine-learning/how-to-access-data) documentation. Please create a container before running the code below. AzureML Datastore functions DOES NOT create a container for you; it merely registers the datastore to be used later. \n",
    "\n",
    "A Datastore is a place where data can be stored that is then made accessible for training or forecasting. Please refer to [Datastore documentation](https://docs.microsoft.com/python/api/azureml-core/azureml.core.datastore(class)) on how to access data from Datastore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new datastore\n",
    "from azureml.core import Datastore\n",
    "\n",
    "blob_datastore_name='energy' # Name of the datastore to workspace\n",
    "container_name=os.getenv(\"BLOB_CONTAINER\", \"energytest\") # Name of Azure blob container\n",
    "account_name=os.getenv(\"BLOB_ACCOUNTNAME\", \"<StorageAccountName>\") # Storage account name\n",
    "account_key=os.getenv(\"BLOB_ACCOUNT_KEY\", \"<storageaccountkey>\") # Storage account access key\n",
    "\n",
    "blob_datastore = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                         datastore_name=blob_datastore_name, \n",
    "                                                         container_name=container_name, \n",
    "                                                         account_name=account_name,\n",
    "                                                         account_key=account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to use AzureData Lake as a Datastore; here is a code example below. Please note that you'd need to create a ServicePrinicpal to access the data on the Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adlsgen2_datastore_name = 'adlsgen2datastore'\n",
    "\n",
    "subscription_id=os.getenv(\"ADL_SUBSCRIPTION\", \"<my_subscription_id>\") # subscription id of ADLS account\n",
    "resource_group=os.getenv(\"ADL_RESOURCE_GROUP\", \"<my_resource_group>\") # resource group of ADLS account\n",
    "\n",
    "account_name=os.getenv(\"ADLSGEN2_ACCOUNTNAME\", \"<my_account_name>\") # ADLS Gen2 account name\n",
    "tenant_id=os.getenv(\"ADLSGEN2_TENANT\", \"<my_tenant_id>\") # tenant id of service principal\n",
    "client_id=os.getenv(\"ADLSGEN2_CLIENTID\", \"<my_client_id>\") # client id of service principal\n",
    "client_secret=os.getenv(\"ADLSGEN2_CLIENT_SECRET\", \"<my_client_secret>\") # the secret of service principal\n",
    "\n",
    "adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(workspace=ws,\n",
    "                                                             datastore_name=adlsgen2_datastore_name,\n",
    "                                                             account_name=account_name, # ADLS Gen2 account name\n",
    "                                                             filesystem='test', # ADLS Gen2 filesystem\n",
    "                                                             tenant_id=tenant_id, # tenant id of service principal\n",
    "                                                             client_id=client_id, # client id of service principal\n",
    "                                                             client_secret=client_secret) # the secret of service principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to default datastore\n",
    "# datastore = ws.get_default_datastore()\n",
    "#or connect to the external blob_store\n",
    "\n",
    "datastore = blob_datastore\n",
    "\n",
    "target_path = 'energy'\n",
    "\n",
    "# Upload train data\n",
    "ds_train_path = target_path + '_train'\n",
    "datastore.upload(src_dir=train_path, target_path=ds_train_path, overwrite=True)\n",
    "\n",
    "# Upload inference data\n",
    "ds_inference_path = target_path + '_inference'\n",
    "datastore.upload(src_dir=inference_path, target_path=ds_inference_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *[Optional]* If data is already in Azure: create Datastore from it\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"color:red\">\n",
    "If your data is already in Azure you don't need to upload it from your local machine to the default datastore. Instead, you can create a new Datastore that references that set of data. \n",
    "The following is an example of how to set up a Datastore from a container in Blob storage where the sample data is located. \n",
    "\n",
    "In this case, the orange juice data is available in a public blob container, defined by the information below. In your case, you'll need to specify the account credentials as well. For more information check [the documentation](https://docs.microsoft.com/python/api/azureml-core/azureml.core.datastore.datastore#register-azure-blob-container-workspace--datastore-name--container-name--account-name--sas-token-none--account-key-none--protocol-none--endpoint-none--overwrite-false--create-if-not-exists-false--skip-validation-false--blob-cache-timeout-none--grant-workspace-access-false--subscription-id-none--resource-group-none-).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "automl"
    ]
   },
   "outputs": [],
   "source": [
    "# blob_datastore_name = \"automl_many_models\"\n",
    "# container_name = \"automl-sample-notebook-data\"\n",
    "# account_name = \"automlsamplenotebookdata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "automl"
    ]
   },
   "outputs": [],
   "source": [
    "# from azureml.core import Datastore\n",
    "\n",
    "# datastore = Datastore.register_azure_blob_container(\n",
    "#     workspace=ws, \n",
    "#     datastore_name=blob_datastore_name, \n",
    "#     container_name=container_name,\n",
    "#     account_name=account_name,\n",
    "#     create_if_not_exists=True\n",
    "# )\n",
    "\n",
    "# if 0 < dataset_maxfiles < 11973:\n",
    "#     ds_train_path = 'oj_data_small/'\n",
    "#     ds_inference_path = 'oj_inference_small/'\n",
    "# else:\n",
    "#     ds_train_path = 'oj_data/'\n",
    "#     ds_inference_path = 'oj_inference/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Register dataset in AML Workspace\n",
    "\n",
    "The last step is creating and registering [datasets](https://docs.microsoft.com/azure/machine-learning/concept-data#datasets) in Azure Machine Learning for the train and inference sets.\n",
    "\n",
    "Using a [FileDataset](https://docs.microsoft.com/python/api/azureml-core/azureml.data.file_dataset.filedataset) is currently the best way to take advantage of the many models pattern, so we create FileDatasets in the next cell. We then [register](https://docs.microsoft.com/azure/machine-learning/how-to-create-register-datasets#register-datasets) the FileDatasets in your Workspace; this associates the train/inference sets with simple names that can be easily referred to later on when we train models and produce forecasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "# Create file datasets\n",
    "ds_train = Dataset.File.from_files(path=datastore.path(ds_train_path), validate=False)\n",
    "ds_inference = Dataset.File.from_files(path=datastore.path(ds_inference_path), validate=False)\n",
    "\n",
    "# Register the file datasets\n",
    "dataset_name = 'energy50'\n",
    "train_dataset_name = dataset_name + '_train'\n",
    "inference_dataset_name = dataset_name + '_inference'\n",
    "ds_train.register(ws, train_dataset_name, create_new_version=True)\n",
    "ds_inference.register(ws, inference_dataset_name, create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 *[Optional]* Interact with the registered dataset\n",
    "\n",
    "After registering the data, it can be easily called using the command below. This is how the datasets will be accessed in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_ds = Dataset.get_by_name(ws, name=train_dataset_name)\n",
    "energy_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to download the data from the registered dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_paths = energy_ds.take(5).download()\n",
    "download_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load one of the data files to see the format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_data = pd.read_csv(download_paths[0])\n",
    "sample_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have created your datasets, you are ready to move to one of the training notebooks to train and score the models:\n",
    "\n",
    "- Automated ML: please open [02_AutoML_Training_Pipeline.ipynb](Automated_ML/02_AutoML_Training_Pipeline/02_AutoML_Training_Pipeline.ipynb).\n",
    "- Custom Script: please open [02_CustomScript_Training_Pipeline.ipynb](Custom_Script/02_CustomScript_Training_Pipeline.ipynb)."
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "roastala"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
